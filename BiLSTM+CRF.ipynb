{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n7VjZPZrKht"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# more libraries probably... \n",
        "import tensorflow\n",
        "\n",
        "import textblob\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAOWCtXMrR4L",
        "outputId": "b50010d5-ae1c-474a-c8c2-62fef1acb071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJpjZb_0wZDb",
        "outputId": "ba090353-6973-47b8-fbba-9eb74d425cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5qg-9VrshUA"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LElRT8G3fZT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluation metric\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import os.path\n",
        "from scipy.stats import sem\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "\n",
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set) + len(gold_set)\n",
        "    return float(nom)/float(denom)\n",
        "\n",
        "\n",
        "def evaluate(pred, gold):\n",
        "    \"\"\"\n",
        "    Based on https://github.com/felipebravom/EmoInt/blob/master/codalab/scoring_program/evaluation.py\n",
        "    :param pred: file with predictions\n",
        "    :param gold: file with ground truth\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # # read the predictions\n",
        "    # pred_lines = pred.readlines()\n",
        "    # # read the ground truth\n",
        "    # gold_lines = gold.readlines()\n",
        "\n",
        "    pred_lines = pred\n",
        "    gold_lines = gold\n",
        "\n",
        "    # only when the same number of lines exists\n",
        "    if (len(pred_lines) == len(gold_lines)):\n",
        "        data_dic = {}\n",
        "        for n, line in enumerate(gold_lines):\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                data_dic[int(parts[0])] = [literal_eval(parts[1])]\n",
        "            else:\n",
        "                raise ValueError('Format problem for gold line %d.', n)\n",
        "\n",
        "        for n, line in enumerate(pred_lines):\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                if int(parts[0]) in data_dic:\n",
        "                    try:\n",
        "                        data_dic[int(parts[0])].append(literal_eval(parts[1]))\n",
        "                    except ValueError:\n",
        "                        # Invalid predictions are replaced by a default value\n",
        "                        data_dic[int(parts[0])].append([])\n",
        "                else:\n",
        "                    raise ValueError('Invalid text id for pred line %d.', n)\n",
        "            else:\n",
        "                raise ValueError('Format problem for pred line %d.', n)\n",
        "\n",
        "        # lists storing gold and prediction scores\n",
        "        scores = []\n",
        "        for id in data_dic:\n",
        "            if len(data_dic[id]) == 2:\n",
        "                gold_spans = data_dic[id][0]\n",
        "                pred_spans = data_dic[id][1]\n",
        "                scores.append(f1(pred_spans, gold_spans))\n",
        "            else:\n",
        "                sys.exit('Repeated id in test data.')\n",
        "\n",
        "        return (np.mean(scores), sem(scores))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hrgu9SZBrSDp"
      },
      "outputs": [],
      "source": [
        "#max comment length\n",
        "max_len = 128 \n",
        "\n",
        "# max embedding dimension(for one word)\n",
        "embedding_dim = 25 \n",
        "# Max feature\n",
        "max_feature = 10000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFvZUJvuuspp"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBSy1aCDrSL5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Read data\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "\n",
        "data = pd.read_csv(\"ML_Final_Proj1/data1/tsd_train.csv\")\n",
        "dev = pd.read_csv(\"ML_Final_Proj1/data1/tsd_trial.csv\")\n",
        "test = pd.read_csv(\"ML_Final_Proj1/data1/tsd_test.csv\")\n",
        "\n",
        "text_data = data['text'].values\n",
        "spans = data['spans'].apply(literal_eval)\n",
        "lbl = [1 if len(s) > 0 else 0 for s in spans]\n",
        "\n",
        "text_data_test = test['text'].values\n",
        "spans_test = test['spans'].apply(literal_eval)\n",
        "test_id = test.index\n",
        "lbl_test = [1 if len(s) > 0 else 0 for s in spans_test]\n",
        "\n",
        "text_data_dev = dev['text'].values\n",
        "spans_dev = dev['spans'].apply(literal_eval)\n",
        "dev_id = dev.index\n",
        "lbl_dev = [1 if len(s) > 0 else 0 for s in spans_dev]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gl92vj90cQx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Token level \n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "tknzr2 = TweetTokenizer()\n",
        "\n",
        "def custom_tokenizer(text_data):\n",
        "    return tknzr2.tokenize(text_data)\n",
        "\n",
        "def retrieve_word_from_span(lst_span, text):\n",
        "    i = 0\n",
        "    token = []\n",
        "    a = 0\n",
        "\n",
        "    word = []\n",
        "\n",
        "    while (i < (len(lst_span) - 1)):\n",
        "        if (lst_span[i] != (lst_span[i+1]-1)):\n",
        "            token.append(lst_span[a:(i+1)])\n",
        "            a = i + 1\n",
        "        elif i == (len(lst_span) - 2):\n",
        "            token.append(lst_span[a:i+2])\n",
        "\n",
        "        i = i + 1\n",
        "\n",
        "    for t in token:\n",
        "        word.append(text[t[0]:(t[len(t)-1])+1])\n",
        "\n",
        "    return word\n",
        "\n",
        "def span_retrived(text_data, spans):\n",
        "    token_labels = []\n",
        "\n",
        "    for i in range(0, len(text_data)):\n",
        "        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n",
        "    \n",
        "    return token_labels\n",
        "\n",
        "def span_convert(text_data, spans):\n",
        "    MAX_LEN = 0\n",
        "    token_labels = []\n",
        "\n",
        "    for i in range(0, len(text_data)):\n",
        "        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n",
        "\n",
        "    lst_seq = []\n",
        "    for i in range(0, len(text_data)):\n",
        "        # token = tknzr.tokenize(text_data[i])\n",
        "        token = custom_tokenizer(text_data[i])\n",
        "        if len(token) > MAX_LEN:\n",
        "            MAX_LEN = len(token)\n",
        "                   \n",
        "        seq = np.zeros(len(token), dtype=int)\n",
        "        for j in range(0, len(token)):\n",
        "            for t in token_labels[i]:\n",
        "                # if token[j] in tknzr.tokenize(t):\n",
        "                if token[j] in custom_tokenizer(t):\n",
        "                    seq[j] = 1\n",
        "        lst_seq.append(seq)     \n",
        "\n",
        "    return (token_labels, lst_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oude7cDn0dap"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# convert data\n",
        "data['token'], data['seq'] = span_convert(text_data, spans)\n",
        "dev['token'], dev['seq'] = span_convert(text_data_dev, spans_dev)\n",
        "test['token'], test['seq'] = span_convert(text_data_test, spans_test)\n",
        "\n",
        "train = deepcopy(data)\n",
        "data = pd.concat([data, dev])\n",
        "     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPl99TY83mXS"
      },
      "source": [
        "# Embedding words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNNlTUkS0CYU",
        "outputId": "3d7e7184-591b-4370-b9ef-22c9815784b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe success\n"
          ]
        }
      ],
      "source": [
        "# Read embedding\n",
        "word_dict = []\n",
        "embeddings_index = {}\n",
        "f = open('ML_Final_Proj1/data1/glove.twitter.27B.25d.txt')\n",
        "for line in f:\n",
        "    values = line.split(' ')\n",
        "    word = values[0] \n",
        "    word_dict.append(word)\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('GloVe success')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k09Ozya70CZ0"
      },
      "outputs": [],
      "source": [
        "words = word_dict\n",
        "num_words = len(words)\n",
        "\n",
        "# Dictionary word:index pair\n",
        "# word is key and its value is corresponding index\n",
        "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
        "word_to_index[\"UNK\"] = 1\n",
        "word_to_index[\"PAD\"] = 0\n",
        "\n",
        "# Dictionary lable:index pair\n",
        "idx2word = {i: w for w, i in word_to_index.items()}\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW3vjoBgwJN6"
      },
      "outputs": [],
      "source": [
        "# first create a matrix of zeros, this is our embedding matrix\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "# for each word in out tokenizer lets try to find that work in our w2v model\n",
        "for word, i in word_to_index.items():\n",
        "    if i > max_feature:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # we found the word - add that words vector to the matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # doesn't exist, assign a random vector\n",
        "        embedding_matrix[i] = np.random.randn(embedding_dim)\n",
        "     \n",
        "\n",
        " # mapping for token cases\n",
        "case2Idx = {'1': 1, '0': 0}\n",
        "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
        "\n",
        "char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
        "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\%$`&=*+@^~|<>\":\n",
        "    char2Idx[c] = len(char2Idx)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpHlcWVGDhSb"
      },
      "outputs": [],
      "source": [
        "import keras.preprocessing.sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RrXV1-RDh0C"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhC4v0pIDc7s",
        "outputId": "ea6e0bcf-84e9-47a4-d273-5b3b827ecbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = data['seq']\n",
        "X = data['text']\n",
        "\n",
        "y_test = test['seq']\n",
        "X_test = test['text']\n",
        "     \n",
        "\n",
        "#train test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size = 0.1)\n",
        "     \n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.initializers import Constant\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "tknzr2 = TweetTokenizer()\n",
        "\n",
        "def custom_tokenizer(text_data):\n",
        "    text_data = text_data.lower()\n",
        "    return tknzr2.tokenize(text_data)\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    word_list = nltk.word_tokenize(text)\n",
        "    lemma = WordNetLemmatizer()\n",
        "\n",
        "    for w in word_list:\n",
        "        w = lemma.lemmatize(w)\n",
        "\n",
        "    new_text = \"\"\n",
        "    for w in word_list:\n",
        "        new_text = new_text + \" \" + w\n",
        "\n",
        "    return new_text\n",
        "\n",
        "def encoding(X, y, isTest = True):\n",
        "    sentences = []\n",
        "    \n",
        "    for t in X:\n",
        "        sentences.append(custom_tokenizer(t))\n",
        "\n",
        "    X = []\n",
        "    for s in sentences:\n",
        "        sent = []\n",
        "        for w in s:\n",
        "            try:\n",
        "                w = w.lower()\n",
        "                sent.append(word_to_index[w])\n",
        "            except:\n",
        "                sent.append(word_to_index[\"UNK\"])\n",
        "        X.append(sent)\n",
        "           \n",
        "    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n",
        "\n",
        "    if isTest:\n",
        "        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=word_to_index[\"PAD\"])\n",
        "        y = to_categorical(y, num_classes=2)\n",
        "    else:\n",
        "        y = None\n",
        "\n",
        "    return (X,y)\n",
        "def encoding1(X,  isTest = True):\n",
        "    sentences = []\n",
        "    \n",
        "    for t in X:\n",
        "        sentences.append(custom_tokenizer(t))\n",
        "\n",
        "    X = []\n",
        "    for s in sentences:\n",
        "        sent = []\n",
        "        for w in s:\n",
        "            try:\n",
        "                w = w.lower()\n",
        "                sent.append(word_to_index[w])\n",
        "            except:\n",
        "                sent.append(word_to_index[\"UNK\"])\n",
        "        X.append(sent)\n",
        "           \n",
        "    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n",
        "\n",
        "  \n",
        "\n",
        "    return X\n",
        "\n",
        "def decoding(text_data, encoding_text, prediction):\n",
        "    test = [[idx2word[i] for i in row] for row in encoding_text]\n",
        "\n",
        "    lst_token = []\n",
        "\n",
        "    for t in range(0, len(test)):\n",
        "        yy_pred = []\n",
        "        for i in range(0, len(test[t])):\n",
        "            if prediction[t][i] == 1:\n",
        "                yy_pred.append(test[t][i])\n",
        "        lst_token.append(yy_pred)\n",
        "\n",
        "    lis_idx = []\n",
        "    for i in range(0, len(text_data)):\n",
        "        idx = []\n",
        "        for t in lst_token[i]:\n",
        "            index = text_data[i].find(t)\n",
        "            idx.append(index)\n",
        "            for j in range(1, len(t)):\n",
        "                index = index + 1\n",
        "                idx.append(index)\n",
        "        lis_idx.append(idx)\n",
        "\n",
        "    return lis_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0S4kNK4Dc-2"
      },
      "outputs": [],
      "source": [
        "X1, y1 = encoding(X_train, y_train)\n",
        "X2, y2 = encoding(X_dev, y_dev)\n",
        "X3, y3 = encoding(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U_O8ImuDdDl"
      },
      "outputs": [],
      "source": [
        "x_t, y_t = encoding(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "681NRCu0Efqz",
        "outputId": "666f2167-6c29-4b22-98f4-fcce0056b043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-ygmak00e\n",
            "  Running command git clone --filter=blob:none --quiet https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-ygmak00e\n",
            "  warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n",
            "  Resolved https://www.github.com/keras-team/keras-contrib.git to commit 3fc5ef709e061416f4bc8a92ca3750c824b5d2b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-contrib==2.0.8) (2.12.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101078 sha256=68b8231189c3e75051452a5d39cd4e990e9c60c8dbed324d51d4c2522a52bbb8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fp79sgbc/wheels/74/d5/f7/0245af7ac33d5b0c2e095688649916e4bf9a8d6b3362a849f5\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P4_jwSjR19U"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def binary_loss(y_true, y_pred):\n",
        "  loss = -1 * y_true * K.log( K.clip(y_pred + K.epsilon(), 0.0, 1.0) )\n",
        "  loss += -1 * (1.0-y_true) * K.log( K.clip(1.0-y_pred+K.epsilon(), 0.0, 1.0) )\n",
        "  return loss\n",
        "     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDn0lGvrR8rc"
      },
      "source": [
        "Conditional Random Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT8f8forR0cl",
        "outputId": "57ef3abd-6acb-4435-9e02-611e460eb597"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HZwqcOXEF66",
        "outputId": "a583ef9a-e957-480b-a155-b8a4bc6dd7c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 128, 25)           29837875  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128, 25)           0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128, 256)         157696    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 128, 128)         32896     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128, 128)          16512     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128, 128)          0         \n",
            "                                                                 \n",
            " crf_1 (CRF)                 [(None, 128),             266       \n",
            "                              (None, 128, 2),                    \n",
            "                              (None,),                           \n",
            "                              (2, 2)]                            \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128, 2)            6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,045,251\n",
            "Trainable params: 30,045,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-30-46b9172317a5>\", line 35, in <cell line: 35>\n",
            "    plot_model(model,to_file=\"bilstm-crf.pdf\",show_shapes=True,show_layer_names=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/vis_utils.py\", line 487, in plot_model\n",
            "    dot.write(to_file, format=extension)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydot_ng/__init__.py\", line 1755, in write\n",
            "    fobj, close = get_fobj(path, 'w+b')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydot_ng/__init__.py\", line 136, in get_fobj\n",
            "    fobj = open(fname, mode)\n",
            "OSError: [Errno 107] Transport endpoint is not connected: 'bilstm-crf.pdf'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-30-46b9172317a5>\", line 35, in <cell line: 35>\n",
            "    plot_model(model,to_file=\"bilstm-crf.pdf\",show_shapes=True,show_layer_names=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/vis_utils.py\", line 487, in plot_model\n",
            "    dot.write(to_file, format=extension)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydot_ng/__init__.py\", line 1755, in write\n",
            "    fobj, close = get_fobj(path, 'w+b')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydot_ng/__init__.py\", line 136, in get_fobj\n",
            "    fobj = open(fname, mode)\n",
            "OSError: [Errno 107] Transport endpoint is not connected: 'bilstm-crf.pdf'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-30-46b9172317a5>\", line 35, in <cell line: 35>\n",
            "    plot_model(model,to_file=\"bilstm-crf.pdf\",show_shapes=True,show_layer_names=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/vis_utils.py\", line 487, in plot_model\n",
            "    dot.write(to_file, format=extension)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydot_ng/__init__.py\", line 1755, in write\n",
            "    fobj, close = get_fobj(path, 'w+b')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydot_ng/__init__.py\", line 136, in get_fobj\n",
            "    fobj = open(fname, mode)\n",
            "OSError: [Errno 107] Transport endpoint is not connected: 'bilstm-crf.pdf'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "from keras import Input\n",
        "from tensorflow_addons.layers import CRF \n",
        "from tensorflow_addons.losses import SigmoidFocalCrossEntropy\n",
        "from keras.utils import plot_model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow_addons\n",
        "# from keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n",
        "\n",
        "input = Input(shape = (max_len,))\n",
        "model = Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_dim,\n",
        "                    embeddings_initializer=Constant(embedding_matrix),\n",
        "                    input_length=max_len,\n",
        "                    trainable=True)(input)\n",
        "\n",
        "model = Dropout(0.1)(model)\n",
        "model = Bidirectional(LSTM(units = max_len, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "model = TimeDistributed(Dense(max_len, activation=\"relu\"))(model)\n",
        "model = Dense(128, activation=\"sigmoid\")(model)\n",
        "model = Dropout(0.1)(model)\n",
        "\n",
        "crff =CRF(2)\n",
        "decoded_sequence, potentials, sequence_length, chain_kernel = crff(model)\n",
        "out = Dense(2, activation=\"sigmoid\")(potentials)\n",
        "\n",
        "\n",
        "model = Model(input, out)\n",
        "model.compile(optimizer=\"adam\",loss = tensorflow_addons.losses.SigmoidFocalCrossEntropy(), metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "plot_model(model,to_file=\"bilstm-crf.pdf\",show_shapes=True,show_layer_names=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "Z_PtbWqoMmiE",
        "outputId": "da4a6a23-7d4e-4fdc-adbb-dac2bc2d51ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f76ed47a9aa9>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                        monitor='val_loss')\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "from keras import Input\n",
        "from tensorflow_addons.layers import CRF \n",
        "from tensorflow_addons.losses import SigmoidFocalCrossEntropy\n",
        "from keras.utils import plot_model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow_addons\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath = '\"ML_Final_Proj1/data1/model/model_detection_2.h5',\n",
        "                       verbose = 0,\n",
        "                       mode = 'auto',\n",
        "                       save_best_only = True,\n",
        "                       monitor='val_loss')\n",
        "\n",
        "model.fit(X1, np.array(y1), batch_size=64, epochs=7, validation_data=(X2, y2), callbacks=[checkpointer])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0QJW4hMNeIK"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('\"ML_Final_Proj1/data1/model/model_detection_1.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-DHjbP6Mmp6",
        "outputId": "698dbdae-942f-4a8d-ceaa-a5646a5c4d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 11s 152ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(X3)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_test_true = np.argmax(y3, -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "4v8heIYmJsuD",
        "outputId": "0c8780e5-fb34-4bae-cef3-d221061d313b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-7c089659f510>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0myy_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0myy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X3' is not defined"
          ]
        }
      ],
      "source": [
        "test = [[idx2word[i] for i in row] for row in X3]\n",
        "     \n",
        "\n",
        "yy_pred = []\n",
        "yy_test = []\n",
        "\n",
        "for i in range(0, len(test[0])):\n",
        "    if y_pred[0][i] == 1:\n",
        "        yy_pred.append(test[0][i])\n",
        "\n",
        "for i in range(0, len(test[0])):\n",
        "    if y_test_true[0][i] == 1:\n",
        "        yy_test.append(test[0][i])\n",
        "\n",
        "print(yy_pred)\n",
        "print(yy_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLtIswkNJuWr",
        "outputId": "ba3b9295-7e73-4c86-e9aa-e393b7ba2065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57.13968131011541\n"
          ]
        }
      ],
      "source": [
        "raw_y = decoding(X_test, X3, y_pred)\n",
        "     \n",
        "\n",
        "f1(raw_y[0], spans_test[0])\n",
        "\n",
        "acc = []\n",
        "for i in range(0, len(spans_test)):\n",
        "    acc.append(f1(raw_y[i], spans_test[i]))\n",
        "\n",
        "print(np.mean(acc)*100)\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  x = input()\n",
        "  xx = encoding1(x)\n",
        "  j=0\n",
        "  xxx = []\n",
        "  for i in xx:\n",
        "    \n",
        "    xxx[i] =idx2word[i] \n",
        "    j+=1\n",
        "  print(xxx.shape)\n",
        "  y_pred = model.predict(xx)\n",
        "  print(y_pred)\n",
        "  y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "     \n",
        "\n",
        "  yy_pred = []\n",
        "  yy_test = []\n",
        "\n",
        "  for i in range(0, len(xxx)):\n",
        "    if y_pred[0][i] == 1:\n",
        "        yy_pred.append(xxx[i])\n",
        "  print(yy_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "1AGe1uAp7hnQ",
        "outputId": "e6210cc4-ba83-4ff8-f983-b4c793ffd3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "salam\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-666c1580754f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mxxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mj\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
